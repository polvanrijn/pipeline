---
title: "Sentence wise re-sampling"
subtitle: "Re-sample emotional prosody to neutral baseline"
author: "Pol van Rijn"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
editor_options: 
  chunk_output_type: console
bibliography: /Users/pol.van-rijn/MPI/02_running_projects/02_PhD/Papers/dynamic_features/References/library.bib
---
<link rel="stylesheet" href="/Users/pol.van-rijn/MPI/04_misc/R/Tufte/extra.css"/>
```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy = FALSE, echo = T, results = 'hide', message = FALSE)
```

# Goal
Before we start programming, let's focus on the goal. The main idea is to re-sample the pitch contours of emotional prosody to it's neutral version. This means we'll need to realign each sentence of each speaker in each corpus to a neutral version. We'll do this so we can apply frequency analysis to each sentence.

Based on previous experience, it does not make sense to re-sample on a word level, there are too few pitch points per word. We therefore re-sample on a sentence level, however we use the pitch information on a word level.

# Approach
Before we start it makes sense to think about our data structure. First of all, we have multiple corpora. Each file in our corpora can vary in these regards:
- the *sentence* read by the participant,
- the *emotion*,
- the *speaker* who reads the sentence,
- the *intensity* with which the speaker speaks (only for some corpora, i.e. high or low intensity),
- the *repetition*, indicating how often this same sentence $\times$ emotion $\times$ speaker $\times$ intensity pair was recorded

Especially the last two regards are problematic. The good news is that all neutral utterances occur in a 'normal' intensity, this means all emotionally differently intense will be squeezed to neutral. The second regard, repetition, is more problematic, since how do you decide you scale which repetition to the other repitition? We decided to scale repetition 1 to 1 and 2 to 2, however this is somewhat arbitrary... However this only occurs in the *RAVDESS* corpus.

Load the packages we'll need
```{r}
library(stringr)
library(dplyr)
library(contouR)
library(ggplot2)

path = paste0( Sys.getenv("PT_DIR"), "Corpora/")
source(paste0( Sys.getenv("PT_DIR"), 'Scripts/99_helpers/resample_sentence_wise.R'))
```

Let's now compute the sentence wise resampling for the manually checked file

```{r percentiles, cache=TRUE}
# Test if it is equal
sentence_wide_df = resample_sentence_wise(path, 'PELL', '08_manual_corrected', '04_completed', paste0(path, "PELL/", "PitchTiers/pitch_settings_old.csv"))
```

This function can help us to test equallity across columns in dataframes
```{r equality_function, cache=TRUE}
check_equality = function(df1, df2, column, filename){
  if (length(df1[[column]]) != length(df2[[column]])) {
    stop("Two time series are not of equal length")
  }
  
  if (typeof(df1[[column]]) != typeof(df2[[column]])) {
    stop("Two time series are not of same type")
  }
  
  if (any(c(is.na(df1[[column]]), is.na(df2[[column]])))) {
    warning(paste(filename, "contains NAs for", column))
  } else{
    
    if (is.numeric(df1[[column]])) {
      equality = all(round(df1[[column]], 4) == round(df1[[column]], 4))
    } else{
      equality = all(sort(unique(df1[[column]])) == sort(unique(df1[[column]])))
    }
    
    if (!equality) {
      stop(paste(filename, "for", column, "not equal"))
    }
  }
  return(!equality)
}
```

Now we can load the old file. We can recode it so we can easily match it.
```{r rename_old_df, cache=TRUE}
old_sentence_wide_df = read.csv('sentence_wise_resamp.csv')

# Compute start of a fragment
start_idx = which(diff(old_sentence_wide_df$t) < 0)
end_idx = c(start_idx, nrow(old_sentence_wide_df))  - 1
start_idx = c(0, start_idx)
IDs = c()
for (i in 1:length(end_idx)) {
  diff_idx = end_idx[i] - start_idx[i] + 1
  IDs = c(IDs, rep(i, diff_idx))
}
old_sentence_wide_df$ID = IDs

# Recode sentence
old_sentence_wide_df$sentence = stringr::str_pad(old_sentence_wide_df$sentence, 2, pad = "0")

# Rename speaker
old_sentence_wide_df$speaker = factor(old_sentence_wide_df$speaker)
levels(old_sentence_wide_df$speaker) = c("DF", "MG", "NA", "SL")

# Rename emotion
names(old_sentence_wide_df)[3] = "emotion"
old_sentence_wide_df$emotion = factor(old_sentence_wide_df$emotion)
levels(old_sentence_wide_df$emotion) = c("NEU", "SUR", "ANG", "DIS", "FER", "HAP", "SAD")

# Set right repetition
old_sentence_wide_df$repetition = 1
rep_2_idxs = filter(old_sentence_wide_df %>% dplyr::group_by(emotion, speaker, sentence) %>% dplyr::summarise(len = length(unique(ID)), idxs = tail(unique(ID), 1)), len == 2)$idxs
old_sentence_wide_df[old_sentence_wide_df$ID %in% rep_2_idxs, "repetition"] = 2

# Update filename
old_sentence_wide_df$filename = paste(old_sentence_wide_df$speaker, old_sentence_wide_df$emotion, old_sentence_wide_df$sentence, "XX", old_sentence_wide_df$repetition, sep = "_")
```

There was a small bug in the previous version, that did not exclude sentences in which sentences with missing pitch for given words were not excluded. We can see 26 files are affected:

```{r compare_filenames, results = '', cache=TRUE}
old_filenames = unique(old_sentence_wide_df$filename)
length(old_filenames)
new_filenames = unique(sentence_wide_df$filename)
length(new_filenames)
old_filenames[!old_filenames %in% new_filenames]
```

For the remaining files the scores are identical:
```{r check_equality, warning=TRUE, cache=TRUE}
for (file_ID in new_filenames) {
  new_df = filter(sentence_wide_df, filename == file_ID)
  old_df = filter(old_sentence_wide_df, filename == file_ID)
  if (any(check_equality(new_df, old_df, "t", file_ID), check_equality(new_df, old_df, "f", file_ID))) {
    stop(paste(file_ID, "are not equal!"))
  }
}
```

```{r, cache=TRUE}
find_missing_files = function(df){
  filenames = unique(df$filename)
  print(length(filenames))
  old_filenames[!old_filenames %in% filenames]
}
```


We can do the same for automatic transcriptions and 
```{r other_resampling, cache=TRUE, warning=FALSE}
new_pitch_wide_df = resample_sentence_wise(path, 'PELL', '02_estimated', '04_completed')
find_missing_files(new_pitch_wide_df)
```

```{r other_resampling_2, cache=TRUE}
new_tg_wide_df = resample_sentence_wise(path, 'PELL', '08_manual_corrected', 'TextGrids', tier_name = "ORT-MAU",  pitch_settings_csv_path = paste0(path, "PELL/", "PitchTiers/pitch_settings_old.csv"))
find_missing_files(new_tg_wide_df)

both_new_wide_df = resample_sentence_wise(path, 'PELL', '02_estimated', 'TextGrids', tier_name = "ORT-MAU")
find_missing_files(both_new_wide_df)
```

```{r do_freq_analysis, cache=TRUE}
do_freq_analysis = function(df, title = NULL, ylim = c(0, 100)){
  padding = ceiling(max(df$t)) + 6
  foi = seq(0, 34, 0.25)
  
  dtM = df %>% filter(speaker %in% c("DF", "MG")) %>% group_by(filename) %>% summarise(f = list(f), t = list(t))
  powspecM = contouR::frequency_analysis(dtM$f, dtM$t, padding, foi)
  
  dtF = df %>% filter(!speaker %in% c("DF", "MG")) %>% group_by(filename) %>% summarise(f = list(f), t = list(t))
  powspecF = contouR::frequency_analysis(dtF$f, dtF$t, padding, foi)
  
  summary_df = rbind(
    data.frame(
      foi = foi,
      amp = colMeans(powspecM),
      gender = "M"
    ),
    data.frame(
      foi = foi,
      amp = colMeans(powspecF),
      gender = "F"
    )
  )
  
  library(ggplot2)
  p = ggplot(summary_df) +
    geom_line(aes(x = foi, y = amp, color = gender)) +
    ylim(ylim)
  
  if (!is.null(title)){
    p = p + ggtitle(title)
  }
  
  return(p)
}
```

```{r compare_data, cache=TRUE}
do_freq_analysis(old_sentence_wide_df, "Original data")
p = do_freq_analysis(sentence_wide_df, "Same data but without 26 files with missing words")
print(p)
```

```{r compare_data2, cache=TRUE}
do_freq_analysis(new_tg_wide_df, "Webmaus TG")
do_freq_analysis(new_pitch_wide_df, "PitchPoints")
do_freq_analysis(both_new_wide_df, "Webmaus TG + PitchPoints")
```

```{r word_wise_vs_naive_resampling, cache=TRUE}
sentence_wide_df_naive = naive_resample_sentence_wise(path, 'PELL', '08_manual_corrected', paste0(path, "PELL/", "PitchTiers/pitch_settings_old.csv"))
find_missing_files(sentence_wide_df_naive)
do_freq_analysis(sentence_wide_df_naive, "Naive computation")

for (file_ID in new_filenames) {
  new_df = filter(sentence_wide_df, filename == file_ID)
  old_df = filter(sentence_wide_df_naive, filename == file_ID)
  if (any(check_equality(new_df, old_df, "t", file_ID), check_equality(new_df, old_df, "f", file_ID))) {
    stop(paste(file_ID, "are not equal!"))
  }
}
```
